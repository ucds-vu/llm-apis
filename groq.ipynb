{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14da2e3-6e78-4099-b85b-670cbfd2db60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5c08b53-1765-4ee4-83ea-41e298bc9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "GROQ_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "406d3c8e-599f-40da-a235-0980629e5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"\"\"\n",
    "Explain the importance of low latency LLMs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d48fe-fff0-4079-bf49-0781a2c7e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b59247d9-da01-4df3-a9e7-23e0520a139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Groq(model=\"llama3-70b-8192\", api_key=GROQ_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4aa0de-76a1-41c0-ba9f-2c1148c803a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\n",
      "\n",
      "1. **Interactive Systems**: In interactive systems like chatbots, virtual assistants, or conversational interfaces, low-latency LLMs enable rapid response times, making the interaction feel more natural and engaging. Users expect quick responses, and high latency can lead to frustration and abandonment.\n",
      "2. **Real-time Decision Making**: In applications like autonomous vehicles, medical diagnosis, or financial trading, low-latency LLMs can process vast amounts of data quickly, enabling rapid decision-making and reaction to changing circumstances.\n",
      "3. **Live Streaming and Broadcasting**: Low-latency LLMs can facilitate real-time language translation, captioning, or subtitling for live events, ensuring that audiences receive timely and accurate information.\n",
      "4. **Gaming and Simulation**: In online gaming and simulation environments, low-latency LLMs can generate realistic NPC (non-player character) interactions, enabling more immersive experiences.\n",
      "5. **Healthcare and Emergency Services**: In healthcare, low-latency LLMs can quickly analyze medical data, provide diagnosis, or generate treatment plans, saving precious time in emergency situations.\n",
      "6. **Customer Service and Support**: Low-latency LLMs can power chatbots and virtual assistants that respond quickly to customer inquiries, improving customer satisfaction and reducing support costs.\n",
      "7. **Edge Computing and IoT**: As IoT devices proliferate, low-latency LLMs can process data closer to the source, reducing latency and enabling real-time processing, analysis, and decision-making at the edge.\n",
      "8. **Security and Surveillance**: Low-latency LLMs can quickly analyze surveillance footage, detect anomalies, and alert authorities, enhancing public safety and security.\n",
      "9. **Accessibility and Inclusion**: Low-latency LLMs can provide real-time language translation, enabling people with disabilities or language barriers to access information and services more easily.\n",
      "10. **Competitive Advantage**: In industries where speed and responsiveness are critical, low-latency LLMs can provide a competitive advantage by enabling faster decision-making, improved customer experiences, and increased operational efficiency.\n",
      "\n",
      "To achieve low latency in LLMs, researchers and developers are exploring various techniques, such as:\n",
      "\n",
      "1. Model pruning and knowledge distillation\n",
      "2. Quantization and precision reduction\n",
      "3. Efficient neural network architectures\n",
      "4. Parallel processing and distributed computing\n",
      "5. Caching and memoization\n",
      "6. Specialized hardware accelerators (e.g., TPUs, GPUs, or FPGAs)\n",
      "\n",
      "By reducing latency, LLMs can unlock new possibilities in various domains, leading to improved user experiences, increased efficiency, and enhanced decision-making capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = llm.complete(user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90ba2070-dd1c-49c0-81e6-b020339e0202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no mention of low latency LLMs (Large Language Models) in the provided context information. The context appears to be a collection of references and citations from various sources, including news articles, websites, and academic publications, covering a wide range of topics such as biographies, sports, entertainment, and more. Therefore, it is not possible to explain the importance of low latency LLMs based on this context.\n"
     ]
    }
   ],
   "source": [
    "model= \"llama3-70b-8192\"\n",
    "collection_name = \"ucds-test\"\n",
    "embedding_model = \"BAAI/bge-small-en-v1.5\"\n",
    "temperature = 1.0\n",
    "\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "try:\n",
    "    chroma_client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "chroma_collection = chroma_client.create_collection(collection_name)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=embedding_model)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./rag_files\").load_data()\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "Settings.llm = Groq(model = model, temperature = temperature)\n",
    "query_engine = index.as_query_engine(llm)\n",
    "response = query_engine.query(user_query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fdb5ef-6539-48bd-a632-6f3db22c5912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
